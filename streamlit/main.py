from transformers import AutoTokenizer, AutoModel
import torch
from collections import Counter
import regex as re
from nltk.tokenize import word_tokenize
from langchain.schema import Document
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever
from chromadb import Settings, EmbeddingFunction, Embeddings
import chromadb
from langchain.vectorstores import Chroma
from ollama import AsyncClient
import asyncio
import ollama
import streamlit as st
import pandas as pd
import numpy as np

st.set_page_config(page_title="CHATBOT RAG app", layout="wide")
st.title("CHATBOT RAG app")
st.write('Welcome to the CHATBOT RAG app!')


base_model_id = "FacebookAI/xlm-roberta-base"
trained_model_path = "./models/trained_embedding_small_data/"
data_path = "data/"
MAX_LEN = 512
OVERLAP = 50

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
model = AutoModel.from_pretrained(trained_model_path)
model.eval()

device = torch.device('cpu')
model.to(device)


class MyEmbeddingFunction(EmbeddingFunction):
    def __init__(self, model, tokenizer, max_length=512):
        self.model = model
        self.tokenizer = tokenizer
        self.max_length = max_length

    def embed_documents(self, texts):
        inputs = self.tokenizer(texts,
                                padding='max_length',
                                max_length=self.max_length,
                                return_tensors="pt")
        with torch.no_grad():
            inputs = {key: value.to(device) for key, value in inputs.items()}
            embeddings = self.model(**inputs).pooler_output

        return embeddings.cpu().numpy().tolist()

    def embed_query(self, text):
        inputs = self.tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            inputs = {key: value.to(device) for key, value in inputs.items()}
            embeddings = self.model(**inputs).pooler_output
        return embeddings.cpu().numpy().tolist()[0]


myembed = MyEmbeddingFunction(model, tokenizer)

db = Chroma(
    embedding_function=myembed,  # l·∫•y embedding model ƒë√£ train ƒë∆∞·ª£c
    persist_directory='./chroma_db',
    collection_name="VNLaws",
)
vectorstore_retriever = db.as_retriever(search_kwargs={"k": 10})

stored_data = db._collection.get()
zip_doc = list(zip(stored_data["documents"], stored_data["metadatas"]))
docs = [Document(page_content=doc, metadata=metadata)
        for doc, metadata in zip_doc]
keyword_retriever = BM25Retriever.from_documents(docs, search_kwargs={"k": 10})
ensemble_retriever = EnsembleRetriever(
    retrievers=[vectorstore_retriever, keyword_retriever], weights=[0.5, 0.5])

ollama_api_url = "https://87ea-35-201-213-122.ngrok-free.app/"  # Thay ƒë·ªïi m·ªói l·∫ßn host

ollama_client = ollama.Client(
    host=ollama_api_url,
    headers={'Header': 'application/json'}
)


system_prompt = "X√¢y d·ª±ng ch∆∞∆°ng tr√¨nh RAG v·ªõi chatbot trung th·ª±c.\n" \
    "V√† kh√¥ng bao gi·ªù tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ch√≠nh tr·ªã, t√¥n gi√°o, t√¨nh d·ª•c v√† c√°c v·∫•n ƒë·ªÅ nh·∫°y c·∫£m kh√°c.\n" \
    "T√¥i c√≥ b·ªô d·ªØ li·ªáu g·ªìm c√°c Lƒ©nh v·ª±c ph√°p lu·∫≠t Vi·ªát Nam bao g·ªìm B·∫£o Hi·ªÉm, Lao ƒê·ªông, Nh√† ƒê·∫•t v√† T√†i Ch√≠nh Ng√¢n H√†ng. " \
    "Trong ƒë√≥ bao b·ªìm c√°c vƒÉn b·∫£n h√†nh ch√≠nh c·ªßa Ch√≠nh ph·ªß.\n"


def judment1_answerable(question, retry=2):
    if retry == 0:
        return None

    agent_judgment1 = 'B·∫°n l√† m·ªôt agent v·ªõi nhi·ªám v·ª• ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi. ' \
        'V·ªõi c√¢u h·ªèi "{question}", b·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng c∆° s·ªü d·ªØ li·ªáu c·ªßa t√¥i ƒë∆∞·ª£c hay kh√¥ng (tr·∫£ l·ªùi 1 n·∫øu c√≥ th·ªÉ, tr·∫£ l·ªùi 0 n·∫øu kh√¥ng th·∫ø). \n' \
        'Tr·∫£ l·ªùi:'

    response = ollama_client.chat(
        model="gemma3:12b",
        messages=[{"role": "user", "content": system_prompt +
                   agent_judgment1.format(question=question)}],
        stream=False,
        options={"temperature": 0.8}
    )
    # print("response.message.content = ", response.message.content)
    if "0" in response.message.content:
        judment1_result = 0
    elif "1" in response.message.content:
        judment1_result = 1
    else:
        return judment1_answerable(question, retry - 1)

    return judment1_result


def planer(question,):
    agent_search_plan = 'B·∫°n l√† m·ªôt agent ƒë·ªÉ d·ª± ƒëo√°n th√¥ng tin c·∫ßn truy v·∫•n.\n' \
        'Theo b·∫°n ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi "{question}", th√¨ c·∫ßn nh·ªØng th√¥ng tin g√¨ ?\n' \
        'G·ª£i √Ω cho t√¥i m·ªôt c√¢u ng·∫Øn g·ªçn b·∫Øt ƒë·∫ßu sau t·ª´ "Tr·∫£ l·ªùi" sau ƒë√¢y.\n' \
        'Tr·∫£ l·ªùi:\n'

    response = ollama_client.chat(
        model="gemma3:12b",
        messages=[{"role": "user", "content": system_prompt +
                   agent_search_plan.format(question=question)}],
        stream=False,
    )
    plan = re.sub("^Tr·∫£ l·ªùi:*\s*", "", response.message.content)
    return plan


def retriever(question, plan, top_k=10):
    text = f"ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi {question} c·∫ßn {plan}"
    relevants = ensemble_retriever.get_relevant_documents(text)
    name = [i.metadata["name"] for i in relevants]
    name_counts = Counter(name)
    ranked_names = sorted(name_counts.items(),
                          key=lambda x: x[1], reverse=True)

    sorted_documents = []
    for name, count in ranked_names:
        for doc in relevants:
            if doc.metadata["name"] == name:
                sorted_documents.append(doc)

    top_documents = sorted_documents[:10]
    return top_documents


PROMP_TEMPLATE = """
    B·∫°n l√† m·ªôt chuy√™n gia tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ lu·∫≠t v·ªÅ c√°c lƒ©nh v·ª±c bao g·ªìm B·∫£o Hi·ªÉm, Lao ƒê·ªông, Nh√† ƒê·∫•t v√† T√†i Ch√≠nh Ng√¢n H√†ng v√† c√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. \n
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu. N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y g·ª£i √Ω c√¢u h·ªèi kh√°c ph√π h·ª£p h∆°n. \n
    ƒê√¢y l√† m·ªôt s·ªë th√¥ng tin b·∫°n c√≥ ƒë∆∞·ª£c:\n
    --------\n
    {context} \n
    --------\n
    D·ª±a v√†o th√¥ng tin tr√™n h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√¢y: {question} \n
    Tr·∫£ l·ªùi: 
    """


messages = st.container()
# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "T√¥i l√† tr·ªü l√Ω ·∫£o, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

st.sidebar.button("Clear Chat History", on_click=lambda: st.session_state.update(
    messages=[{"role": "assistant", "content": "T√¥i l√† tr·ªü l√Ω ·∫£o, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}]))


def generate_stream_response(prompt):
    response = ollama_client.chat(
        model="gemma3:12b",
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        options={"temperature": 0.9}
    )
    for partial_rep in response:
        token = partial_rep.message.content
        st.session_state['full_message'] += token
        yield token


if prompt := st.chat_input("Prompt c·ªßa b·∫°n"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    messages.chat_message('user').write(prompt)

    if judment1_answerable(prompt) == 0:
        st.chat_message("assistant", avatar="ü§ñ").write("Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.")
        st.session_state.messages.append({"role": "assistant", "content": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."})
    if judment1_answerable(prompt) == 1:
        plan = planer(prompt)
        st.chat_message("agent planner", avatar="üß†").write(plan)
        st.session_state.messages.append({"role": "agent planner", "content": plan})

        top_documents = retriever(prompt, plan)
        messages.chat_message("agent retriever", avatar="üîç").write("M·ªôt s·ªë ngu·ªìn tham kh·∫£o")
        references = {doc.metadata["name"]: doc.metadata['url'] for doc in top_documents}
        for item in references.items():
            messages.chat_message("agent reader", avatar="üìö").write(item)

        context = "\n--------\n".join([doc.page_content for doc in top_documents])

        st.session_state['full_message'] = ""
        st.chat_message("assistant", avatar="ü§ñ").write_stream(generate_stream_response(PROMP_TEMPLATE.format(context=context, question=prompt)))
        st.session_state.messages.append(
            {"role": "assistant", "content": st.session_state['full_message']})

    # messages.chat_message("agent retriever", avatar="üîç").write("ƒê√¢y l√† c√¢u tr·∫£ l·ªùi t·ª´ agent retriever")
    #
    # messages.chat_message("agent generator", avatar="üß†").write("ƒê√¢y l√† c√¢u tr·∫£ l·ªùi t·ª´ agent generator")
    # messages.chat_message("ai").write("ƒê√¢y l√† c√¢u tr·∫£ l·ªùi cu·ªëi c√πng")
    # Add a button to clear chat history
