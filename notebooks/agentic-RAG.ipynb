{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\.HCMUS\\NLP2\\master-llm-rag-vnlaw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "while os.getcwd().split('\\\\')[-1] != 'master-llm-rag-vnlaw':\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database và Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Settings, EmbeddingFunction, Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VNLaws']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(\n",
    "    path=\"./data/chroma_db/\",\n",
    "    settings=Settings(allow_reset=True,),)\n",
    "\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20921"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = client.get_collection(name=\"VNLaws\")\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"FacebookAI/xlm-roberta-base\"\n",
    "trained_model_path = \"./models/trained_embedding_small_data/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "OVERLAP = 50\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "model = AutoModel.from_pretrained(trained_model_path)\n",
    "model.eval()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model ready !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer, max_length=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        inputs = self.tokenizer(texts, \n",
    "                                padding='max_length', \n",
    "                                max_length=self.max_length, \n",
    "                                return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            embeddings = self.model(**inputs).pooler_output\n",
    "    \n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            embeddings = self.model(**inputs).pooler_output\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "myembed = MyEmbeddingFunction(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:\n",
      " [[0.82892543]]\n"
     ]
    }
   ],
   "source": [
    "# text = 'Tôi đi học'\n",
    "# with torch.no_grad():\n",
    "#     token = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "#     output = model(**token)\n",
    "\n",
    "output = myembed.embed_documents([\"Tôi đi học\"])\n",
    "output1 = myembed.embed_query(\"Tôi đang viết trên bảng ở trường\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(output.reshape(1, -1), output1.reshape(1, -1))\n",
    "print(f\"Cosine similarity:\\n {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ollama client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_api_url = \"https://6efb-34-141-226-213.ngrok-free.app\" # Thay đổi mỗi lần host\n",
    "\n",
    "ollama_client = ollama.Client(\n",
    "    host=ollama_api_url,\n",
    "    headers = {'Header': 'application/json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Xây dựng chương trình RAG với chatbot trung thực.\\n\" \\\n",
    "\"Tôi có bộ dữ liệu gồm các Lĩnh vực pháp luật Việt Nam bao gồm Bảo Hiểm, Lao Động, Nhà Đất và Tài Chính Ngân Hàng. \" \\\n",
    "\"Trong đó bao bồm các văn bản hành chính của Chính phủ.\\n\"\n",
    "\n",
    "agent_judgment1 = 'Bạn là một agent dùng để đánh giá câu trả lời. ' \\\n",
    "'Với câu hỏi \"{question}\", Bạn có thể trả lời câu hỏi bằng cơ sở dữ liệu của tôi được hay không (trả lời 1 nếu có thể, trả lời 0 nếu không thế).' \\\n",
    "'Trả lời:' \\\n",
    "\n",
    "agent_search_plan = 'Bạn là một agent để dự đoán thông tin cần truy vấn.\\n' \\\n",
    "'Theo bạn để trả lời câu hỏi \"{question}\", thì cần những thông tin gì và đến từ Lĩnh vực nào?\\n' \\\n",
    "'Trả lời tối đa 3 dòng và theo cú pháp \"<linh_vuc>: <thong_tin_can>\", linh_vuc chỉ duy nhất một, 3 câu trả lời cho thể trùng Lĩnh vực nhưng khác nhau thông tin cần.\\n' \\\n",
    "'Trả lời: '\n",
    "\n",
    "agent_judgment2 = 'Bạn là một agent reranking' \\\n",
    "\n",
    "answer_prompt_template = \\\n",
    "    \"\"\"\n",
    "    Bạn là một chuyên gia trả lời câu hỏi về luật về các lĩnh vực bao gồm Bảo Hiểm, Lao Động, Nhà Đất và Tài Chính Ngân Hàng và câu trả lời dựa trên thông tin được cung cấp. Hãy trả lời câu hỏi ngắn gọn và dễ hiểu. Nếu không có thông tin để trả lời câu hỏi, hãy gợi ý câu hỏi khác. Đây là thông tin bạn có được cho câu hỏi này: {context} và chủ đề của câu hỏi {plans}. Dựa vào thông tin có được hãy trả lời câu hỏi sau đây: {question} \\n\n",
    "    Trả lời: \n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đặt câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Quần đảo Trường Sa thuộc tỉnh nào?\"\n",
    "question = \"Tôi có thể làm gì với hợp đồng lao động?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đánh giá khả năng trả lời câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judment1: Trả lời được hay không\n",
    "response = ollama_client.chat(\n",
    "    model=\"gemma3:12b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": system_prompt + agent_judgment1.format(question=question)}],\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lựa chọn thông tin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có thể trả lời được\n"
     ]
    }
   ],
   "source": [
    "def parse_judgment(response_text: str) -> int:\n",
    "    for char in response_text.strip():\n",
    "        if char in [\"0\", \"1\"]:\n",
    "            return int(char)\n",
    "    return 0\n",
    "\n",
    "\n",
    "if parse_judgment(response.message.content) == 1:\n",
    "    print(\"Có thể trả lời được\")\n",
    "\n",
    "    response = ollama_client.chat(\n",
    "        model=\"gemma3:12b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": system_prompt + agent_search_plan.format(question=question),\n",
    "            }\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "    plans = response.message.content.split(\"\\n\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Không thể trả lời được\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT = agent_search_plan.format(question=question)\n",
    "\n",
    "\n",
    "def answer(question, plans):\n",
    "    if isinstance(plans, list):\n",
    "        plans = \", \".join(plans)\n",
    "\n",
    "    # embed the query\n",
    "    query_embed = myembed.embed_query( question + \" \" + plans)\n",
    "    try:\n",
    "        context_list = collection.query(\n",
    "            query_embeddings=query_embed[:768], n_results=5\n",
    "        )[\"documents\"][0]\n",
    "        context = \" \".join(context_list)\n",
    "    except:\n",
    "        try:\n",
    "            context_list = collection.query(\n",
    "                query_embeddings=query_embed[0][:768], n_results=5\n",
    "            )[\"documents\"][0]\n",
    "            context = \" \".join(context_list)\n",
    "        except:\n",
    "            context = \"Không tìm thấy thông tin.\"\n",
    "\n",
    "    answer_prompt = system_prompt + answer_prompt_template.format(\n",
    "        question=question, context=context, plans=plans\n",
    "    )\n",
    "\n",
    "    response = ollama_client.chat(\n",
    "        model=\"gemma3:12b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": answer_prompt,\n",
    "            }\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(question, plans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
