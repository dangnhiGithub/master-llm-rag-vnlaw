{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# while os.getcwd().split('\\\\')[-1] != 'master-llm-rag-vnlaw':\n",
    "while os.getcwd().split('\\\\')[-1] != 'master-llm-rag-vnlaw':\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"FacebookAI/xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/kaggle/input/vn-law-test/data\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "OVERLAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaoHiem.json', 'BoMayHanhChinh.json', 'CoCauToChuc.json', 'CongNgheThongTin.json', 'CongNghiep.json', 'DanSu.json', 'DichVuPhapLy.json', 'DoanhNghiep.json', 'GiaoDuc.json', 'GiaoThongVanTai.json', 'LaoDong.json', 'NhaDat.json', 'TaiChinh', 'TaiChinhNganHang.json']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(data_path)\n",
    "print(files)\n",
    "data_stored = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_phrase_chunking(paragraphs):\n",
    "#     for i in range(len(paragraphs)):\n",
    "#         s = paragraphs[i]\n",
    "#         s = re.sub('CỘNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM', '', s)\n",
    "#         s = re.sub('Độc lập - Tự do - Hạnh phúc', '', s)\n",
    "#         s = re.sub('[- ]+', ' ', s)\n",
    "#         s = s.strip()\n",
    "#         paragraphs[i] = s\n",
    "\n",
    "#     for i in range(3):\n",
    "#         for i in range(len(paragraphs)):\n",
    "#             if len(paragraphs[i]) < 1:\n",
    "#                 paragraphs.pop(i)\n",
    "#                 break\n",
    "    \n",
    "#     return paragraphs\n",
    "\n",
    "# def count_words(input_text):\n",
    "#     return len(input_text.split())\n",
    "\n",
    "# def markdown_chunking(paragraphs, max_len=512, overlap=100):\n",
    "#     lst_chunks = []\n",
    "#     current_chunk = ''\n",
    "#     for p in paragraphs:\n",
    "#         if count_words(current_chunk) + count_words(p) <= max_len:\n",
    "#             if len(current_chunk) == 0:\n",
    "#                 current_chunk = p\n",
    "#             else:\n",
    "#                 current_chunk = \"\\n\".join([current_chunk, p])\n",
    "#         else:\n",
    "#             lst_chunks.append(current_chunk)\n",
    "#             current_chunk = \" \".join(current_chunk.split(\" \")[-100:]) + \"\\n\" + p\n",
    "\n",
    "#     lst_chunks.append(current_chunk)\n",
    "#     return lst_chunks \n",
    "\n",
    "# def tokenizer_chunking(content, tokenizer):\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 1 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[0]\n",
    "\n",
    "with open(f\"{data_path}/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_phrase_chunking(content: str):\n",
    "    content = re.sub('CỘNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM', '', content)\n",
    "    content = re.sub('Độc lập - Tự do - Hạnh phúc', '', content)\n",
    "    content = re.sub('[- ]+', ' ', content)\n",
    "    content = content.strip()\n",
    "    return content\n",
    "\n",
    "content = data['content']\n",
    "content = \"\\n\".join(content)\n",
    "content = preprocess_phrase_chunking(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 570, which is longer than the specified 512\n",
      "Created a chunk of size 518, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HỘI ĐỒNG NHÂN DÂN TỈNH KHÁNH HÒA \n",
      " \n",
      "Số: 06/2024/NQ HĐND\n",
      "Khánh Hòa, ngày 12 tháng 7 năm 2024\n",
      "NGHỊ QUYẾT\n",
      "VỀ VIỆC BAN HÀNH MỨC GIÁ DỊCH VỤ KHÁM BỆNH, CHỮA BỆNH KHÔNG THUỘC PHẠM VI THANH TOÁN CỦA QUỸ BẢO HIỂM Y TẾ MÀ KHÔNG PHẢI LÀ DỊCH VỤ KHÁM BỆNH, CHỮA BỆNH THEO YÊU CẦU TẠI CÁC CƠ SỞ KHÁM BỆNH, CHỮA BỆNH CỦA NHÀ NƯỚC TRÊN ĐỊA BÀN TỈNH KHÁNH HÒA\n",
      "HỘI ĐỒNG NHÂN DÂN TỈNH KHÁNH HÒA KHÓA VII, KỲ HỌP THỨ 14\n",
      "Căn cứ Luật Tổ chức chính quyền địa phương ngày 19 tháng 6 năm 2015;\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(\n",
    "    texts[7], \n",
    "    truncation=True, \n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "tokens['input_ids'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def tokenizer_chunking(content, tokenizer, max_len=512, overlap=50):\n",
    "#     tokens = tokenizer.encode(content, add_special_tokens=False)\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "\n",
    "#     while start < len(tokens):\n",
    "#         end = min(start + max_len, len(tokens))\n",
    "#         chunk = tokens[start:end]\n",
    "#         chunks.append(chunk)\n",
    "#         start += max_len - overlap\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Example usage\n",
    "# tokenized_chunks = tokenizer_chunking(content, tokenizer, max_len=MAX_LEN, overlap=OVERLAP)\n",
    "# print(f\"Number of chunks: {len(tokenized_chunks)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
